{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b94cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d998add",
   "metadata": {},
   "source": [
    "### Summarization MiddleWare\n",
    "\n",
    "Automatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:\n",
    "\n",
    "- Long-running conversations that exceed context windows.\n",
    "- Multi-turn dialogues with extensive history.\n",
    "- Applications where preserving full conversation context matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5213b8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000002A54A068550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.messages import HumanMessage,SystemMessage\n",
    "\n",
    "\n",
    "agent=create_agent(\n",
    "    model=\"groq:qwen/qwen3-32b\",\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"groq:qwen/qwen3-32b\",\n",
    "            trigger=(\"messages\",10),\n",
    "            keep=(\"messages\",4)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4554e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## thread id\n",
    "config={\"configurable\":{\"thread_id\":\"test1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a00ed2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages:content='<think>\\nOkay, the user is asking \"What is 2+2?\" again. Let me check the history. They\\'ve asked this before multiple times, and each time I answered with \"2 plus 2 equals **4**. **Answer:** 4\". Since they\\'re repeating the same question, maybe they want to confirm if the answer is consistent or if I\\'m paying attention. I should stick to the same format to maintain consistency. No need to add extra info unless they ask for more. Just the straightforward answer in bold again.\\n</think>\\n\\n2 + 2 equals **4**.  \\n\\n**Answer:** 4' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 342, 'total_tokens': 470, 'completion_time': 0.275083111, 'completion_tokens_details': None, 'prompt_time': 0.014414104, 'prompt_tokens_details': None, 'queue_time': 0.158833031, 'total_time': 0.289497215}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c3dcd-5dcf-76c0-bf86-802d8006713a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 342, 'output_tokens': 128, 'total_tokens': 470}\n",
      "Messages: 6\n",
      "<class 'dict'>\n",
      "Messages:content='<think>\\nOkay, the user asked \"What is 10*5?\" Let me check the previous interactions to maintain consistency.\\n\\nLooking back, the user has been testing arithmetic problems, and each time I provided the answer in bold followed by \"Answer: X\". The pattern is clear, so I should follow the same structure here.\\n\\n10 multiplied by 5 is straightforward. 10 times 5 equals 50. I need to state the problem, the solution in bold, and then the Answer line. Let me make sure there are no typos. Yep, 10*5 is 50. Ready to respond.\\n</think>\\n\\n10 multiplied by 5 equals **50**.  \\n\\n**Answer:** 50' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 375, 'total_tokens': 528, 'completion_time': 0.281098837, 'completion_tokens_details': None, 'prompt_time': 0.015831562, 'prompt_tokens_details': None, 'queue_time': 0.158129748, 'total_time': 0.296930399}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c3dcd-601f-7c13-8d89-e9cb6467130a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 375, 'output_tokens': 153, 'total_tokens': 528}\n",
      "Messages: 8\n",
      "<class 'dict'>\n",
      "Messages:content='<think>\\nOkay, the user asked \"What is 100/4?\" I need to calculate that. Let\\'s see, 100 divided by 4. Hmm, 4 times 25 is 100, so the answer should be 25. Wait, let me double-check. 4 times 20 is 80, and 4 times 5 is 20, so 80 + 20 = 100. Yep, that\\'s right. So 100 divided by 4 is 25. I should present the answer in the same format as before: state the problem, the solution in bold, and then the Answer line. Let me make sure there\\'s no mistake. 4 multiplied by 25 equals 100, so dividing 100 by 4 gives 25. All set.\\n</think>\\n\\n100 divided by 4 equals **25**.  \\n\\n**Answer:** 25' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 211, 'prompt_tokens': 413, 'total_tokens': 624, 'completion_time': 0.347896848, 'completion_tokens_details': None, 'prompt_time': 0.017563924, 'prompt_tokens_details': None, 'queue_time': 0.157789542, 'total_time': 0.365460772}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c3dcd-622e-78f2-a866-f58988b9de2c-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 413, 'output_tokens': 211, 'total_tokens': 624}\n",
      "Messages: 10\n",
      "<class 'dict'>\n",
      "Messages:content='<think>\\nOkay, let\\'s see. The user is asking \"What is 15-7?\" So, they want to subtract 7 from 15.\\n\\nFirst, I need to calculate 15 minus 7. Let me do the math. 15 minus 7 is 8. That\\'s straightforward.\\n\\nNow, the previous interactions show that the user expects the answer in bold followed by the \"Answer: X\" line. So, I should follow the same format here. The system also appends each answer to the history.json file, but since I don\\'t have access to that file, I just need to make sure the response is correctly formatted.\\n\\nWait, the user might be testing consistency again. They repeated \"2+2\" multiple times, and I answered 4 each time. For \"15-7\", the answer is 8, so I should present it as **8** and then the Answer line.\\n\\nI should double-check the calculation to avoid any mistakes. 15 minus 7: 15 - 7 = 8. Yes, that\\'s correct. No need to overcomplicate it.\\n\\nAlright, the response should be concise and match the established pattern. No extra text, just the bold number and the Answer line.\\n</think>\\n\\n15 minus 7 equals **8**.  \\n\\n**Answer:** 8' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 722, 'total_tokens': 1002, 'completion_time': 0.535571822, 'completion_tokens_details': None, 'prompt_time': 0.036476837, 'prompt_tokens_details': None, 'queue_time': 0.159519986, 'total_time': 0.572048659}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c3dcd-6b00-7930-8e30-f60e7e39b403-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 722, 'output_tokens': 280, 'total_tokens': 1002}\n",
      "Messages: 6\n",
      "<class 'dict'>\n",
      "Messages:content='<think>\\nOkay, so the user asked \"What is 3*3?\" Let me check the history to see the pattern. They\\'ve been asking arithmetic questions like 2+2, 10*5, 100/4, 15-7, and so on. Each time, the assistant answers with the result in bold followed by \"Answer: X\".\\n\\nFirst, calculate 3 multiplied by 3. That\\'s straightforward‚Äî3 times 3 is 9. Need to make sure there\\'s no trick here. The user might be testing if the assistant can handle basic multiplication consistently. Since previous answers were correct, just need to follow the same structure.\\n\\nNext, I have to format the response correctly. The previous examples show the result in bold, then the Answer line. So, \"3*3 equals **9**. Answer: 9\". Also, remember to append this to the JSON file as per the artifacts mentioned. The file path is history.json, and each entry is a JSON object with question and answer. \\n\\nCheck if the user is trying to see if there\\'s any variation in answers. Since they repeated 2+2 multiple times, they want consistency. So, the answer must be exact and in the same format every time. \\n\\nNo other operations are involved here, just a simple multiplication. No need to overcomplicate. Ensure the JSON is updated correctly. Alright, that\\'s it.\\n</think>\\n\\n3 multiplied by 3 equals **9**.  \\n\\n**Answer:** 9' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 312, 'prompt_tokens': 755, 'total_tokens': 1067, 'completion_time': 0.679334581, 'completion_tokens_details': None, 'prompt_time': 0.077979929, 'prompt_tokens_details': None, 'queue_time': 0.047138781, 'total_time': 0.75731451}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c3dcd-6e3d-7660-a60a-fb411a07c1ea-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 755, 'output_tokens': 312, 'total_tokens': 1067}\n",
      "Messages: 8\n",
      "<class 'dict'>\n",
      "Messages:content='<think>\\nOkay, the user is asking \"What is 4*4?\" Let me check the history. They\\'ve been doing multiplication, division, and subtraction problems. The pattern is to answer with the result in bold and then the Answer: line.\\n\\nSo, 4 multiplied by 4 is 16. I need to make sure I use the same structure as before. Let me confirm the calculation again. 4 times 4 is definitely 16. No mistakes there.\\n\\nAlso, the user might be testing consistency, especially since they repeated \"2+2\" earlier. But this is a new problem, so just need to follow the same format. Write out the operation in words, bold the result, then the Answer line. No need for extra explanations. Keep it simple and match the previous responses exactly. \\n\\nI should also remember to update the history.json file with this new question and answer. The user mentioned the artifact, so I need to ensure the JSON is correctly appended. Each entry should have \"question\" and \"answer\" keys. Let me format that correctly in JSON syntax without any errors.\\n\\nDouble-checking everything. The answer is correct, the format matches, and the JSON structure is right. Alright, ready to respond.\\n</think>\\n\\n4 multiplied by 4 equals **16**.  \\n\\n**Answer:** 16' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 275, 'prompt_tokens': 788, 'total_tokens': 1063, 'completion_time': 0.637436732, 'completion_tokens_details': None, 'prompt_time': 0.033831018, 'prompt_tokens_details': None, 'queue_time': 0.159030632, 'total_time': 0.67126775}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c3dcd-79a8-7a52-aec2-ecec73c0bb1a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 788, 'output_tokens': 275, 'total_tokens': 1063}\n",
      "Messages: 10\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Alternative test data\n",
    "questions = [\n",
    "    \"What is 2+2?\",\n",
    "    \"What is 10*5?\",\n",
    "    \"What is 100/4?\",\n",
    "    \"What is 15-7?\",\n",
    "    \"What is 3*3?\",\n",
    "    \"What is 4*4?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response=agent.invoke({\"messages\":[HumanMessage(content=q)]},config)\n",
    "    print(f\"Messages:{response[\"messages\"][-1]}\")\n",
    "    print(f\"Messages: {len(response['messages'])}\")\n",
    "    print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7d1e8",
   "metadata": {},
   "source": [
    "### Token Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "727312c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "@tool\n",
    "def search_hotels(city: str) -> str:\n",
    "    \"\"\"Search hotels - returns long response to use more tokens.\"\"\"\n",
    "    return f\"\"\"Hotels in {city}:\n",
    "    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\n",
    "    2. City Inn - 4 star, $180/night, business center\n",
    "    3. Budget Stay - 3 star, $75/night, free wifi\"\"\"\n",
    "\n",
    "\n",
    "agent=create_agent(\n",
    "    model=\"groq:qwen/qwen3-32b\",\n",
    "    tools=[search_hotels],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"groq:llama3-8b-8192\",\n",
    "            trigger=(\"tokens\",550),\n",
    "            keep=(\"tokens\",200),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"test-1\"}}\n",
    "\n",
    "# Token counter (approximate)\n",
    "def count_tokens(messages):\n",
    "    total_chars = sum(len(str(m.content)) for m in messages)\n",
    "    return total_chars // 4  # 4 chars ‚âà 1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "860e79b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris: ~149 tokens, 4 messages\n",
      "[HumanMessage(content='Find hotels in Paris', additional_kwargs={}, response_metadata={}, id='f19c8e42-6ad8-4698-b7dc-64a11d77e1b5'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user wants to find hotels in Paris. Let me check the available tools. There\\'s a function called search_hotels that takes a city parameter. The description says it returns a long response to use more tokens. Hmm, maybe the response is detailed, which is good for the user. I need to call this function with the city set to Paris. Let me make sure the parameters are correct. The required parameter is city, and it\\'s a string. So the arguments should be {\"city\": \"Paris\"}. I\\'ll format the tool call accordingly.\\n', 'tool_calls': [{'id': 'b4w5j798e', 'function': {'arguments': '{\"city\":\"Paris\"}', 'name': 'search_hotels'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 139, 'prompt_tokens': 155, 'total_tokens': 294, 'completion_time': 0.216461982, 'completion_tokens_details': {'reasoning_tokens': 114}, 'prompt_time': 0.006361725, 'prompt_tokens_details': None, 'queue_time': 0.046844512, 'total_time': 0.222823707}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd6-f70c-7790-8218-37564a34c4b3-0', tool_calls=[{'name': 'search_hotels', 'args': {'city': 'Paris'}, 'id': 'b4w5j798e', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 155, 'output_tokens': 139, 'total_tokens': 294, 'output_token_details': {'reasoning': 114}}), ToolMessage(content='Hotels in Paris:\\n    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\\n    2. City Inn - 4 star, $180/night, business center\\n    3. Budget Stay - 3 star, $75/night, free wifi', name='search_hotels', id='d64f3ada-8af0-40fa-b7dd-45a74201cebf', tool_call_id='b4w5j798e'), AIMessage(content='Here are some hotels in Paris with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Amenities: Spa, pool, gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üõÅ Amenities: Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üõÅ Amenities: Free WiFi  \\n\\nWould you like assistance with booking or more details about any of these? üòä', additional_kwargs={'reasoning_content': \"Okay, the user asked for hotels in Paris, and I called the search_hotels function with Paris as the city. The response came back with three options. Now I need to present this information clearly. Let me list each hotel with their details. Start with a brief intro, then list each one with bullet points or numbers. Mention the name, star rating, price, and amenities. Make sure it's easy to read. Also, maybe offer to book or get more info if they want. Keep the tone friendly and helpful. Check for any typos or formatting issues. Alright, that should cover it.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 267, 'prompt_tokens': 258, 'total_tokens': 525, 'completion_time': 0.469783426, 'completion_tokens_details': {'reasoning_tokens': 123}, 'prompt_time': 0.011358502, 'prompt_tokens_details': None, 'queue_time': 0.169372856, 'total_time': 0.481141928}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd6-f8b4-7de0-9622-e00d25883183-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 258, 'output_tokens': 267, 'total_tokens': 525, 'output_token_details': {'reasoning': 123}})]\n",
      "London: ~343 tokens, 6 messages\n",
      "[HumanMessage(content=\"Here is a summary of the conversation to date:\\n\\nError generating summary: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\", additional_kwargs={'lc_source': 'summarization'}, response_metadata={}, id='1e451eea-cdf7-4f0e-bc1e-3ca4a1c17906'), AIMessage(content='Here are some hotels in Paris with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Amenities: Spa, pool, gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üõÅ Amenities: Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üõÅ Amenities: Free WiFi  \\n\\nWould you like assistance with booking or more details about any of these? üòä', additional_kwargs={'reasoning_content': \"Okay, the user asked for hotels in Paris, and I called the search_hotels function with Paris as the city. The response came back with three options. Now I need to present this information clearly. Let me list each hotel with their details. Start with a brief intro, then list each one with bullet points or numbers. Mention the name, star rating, price, and amenities. Make sure it's easy to read. Also, maybe offer to book or get more info if they want. Keep the tone friendly and helpful. Check for any typos or formatting issues. Alright, that should cover it.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 267, 'prompt_tokens': 258, 'total_tokens': 525, 'completion_time': 0.469783426, 'completion_tokens_details': {'reasoning_tokens': 123}, 'prompt_time': 0.011358502, 'prompt_tokens_details': None, 'queue_time': 0.169372856, 'total_time': 0.481141928}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd6-f8b4-7de0-9622-e00d25883183-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 258, 'output_tokens': 267, 'total_tokens': 525, 'output_token_details': {'reasoning': 123}}), HumanMessage(content='Find hotels in London', additional_kwargs={}, response_metadata={}, id='a18c005a-c210-48c8-a39a-ffd1edd7550b'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user previously asked for hotels in Paris, and I used the search_hotels function to get the results. Now they\\'re asking for hotels in London. I need to follow the same process.\\n\\nFirst, check the tools provided. The search_hotels function requires a city parameter. The user mentioned London, so I should call the function with \"London\" as the city. There\\'s no other parameters needed, just the city name. I need to make sure the function call is correctly formatted in JSON within the tool_call tags. Let me verify the syntax again. The function name is search_hotels, and the arguments should be {\"city\": \"London\"}. No typos, and the city name is properly capitalized. Alright, that should do it. Let me generate the tool call.\\n', 'tool_calls': [{'id': 'jwavcje01', 'function': {'arguments': '{\"city\":\"London\"}', 'name': 'search_hotels'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 186, 'prompt_tokens': 411, 'total_tokens': 597, 'completion_time': 0.29994493, 'completion_tokens_details': {'reasoning_tokens': 161}, 'prompt_time': 0.017909727, 'prompt_tokens_details': None, 'queue_time': 0.046760863, 'total_time': 0.317854657}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd6-fbed-7bc3-867a-a16462979115-0', tool_calls=[{'name': 'search_hotels', 'args': {'city': 'London'}, 'id': 'jwavcje01', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 411, 'output_tokens': 186, 'total_tokens': 597, 'output_token_details': {'reasoning': 161}}), ToolMessage(content='Hotels in London:\\n    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\\n    2. City Inn - 4 star, $180/night, business center\\n    3. Budget Stay - 3 star, $75/night, free wifi', name='search_hotels', id='18bdc65f-fe4a-44f9-b1ac-fdf08fb34b63', tool_call_id='jwavcje01'), AIMessage(content='Here are some hotels in London with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Amenities: Spa, pool, gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üõÅ Amenities: Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üõÅ Amenities: Free WiFi  \\n\\nWould you like assistance with booking or more details about any of these? üòä', additional_kwargs={'reasoning_content': 'Okay, the user asked for hotels in London, and I called the search_hotels function with London as the city. The tool response came back with three hotels similar to the previous Paris example. Now I need to present this information in a user-friendly way.\\n\\nFirst, I should list each hotel with their details. Use emojis for stars and amenities to make it visually appealing. Maybe bullet points for clarity. I should mention the price, star rating, and key amenities. Also, offer further assistance in case they need more details or booking help. Keep the tone friendly and helpful. Let me check if all the information from the tool response is included. Yep, all three hotels with their respective details. Alright, format that into a clear response.\\n'}, response_metadata={'token_usage': {'completion_tokens': 293, 'prompt_tokens': 503, 'total_tokens': 796, 'completion_time': 1.249299742, 'completion_tokens_details': {'reasoning_tokens': 149}, 'prompt_time': 0.023692794, 'prompt_tokens_details': None, 'queue_time': 0.048058559, 'total_time': 1.272992536}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd6-fdfa-7e81-b0c9-10dfbfdbf0d9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 503, 'output_tokens': 293, 'total_tokens': 796, 'output_token_details': {'reasoning': 149}})]\n",
      "Tokyo: ~342 tokens, 6 messages\n",
      "[HumanMessage(content=\"Here is a summary of the conversation to date:\\n\\nError generating summary: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\", additional_kwargs={'lc_source': 'summarization'}, response_metadata={}, id='d1645fa1-3b91-46af-baf2-7a3c62aa404a'), AIMessage(content='Here are some hotels in London with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Amenities: Spa, pool, gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üõÅ Amenities: Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üõÅ Amenities: Free WiFi  \\n\\nWould you like assistance with booking or more details about any of these? üòä', additional_kwargs={'reasoning_content': 'Okay, the user asked for hotels in London, and I called the search_hotels function with London as the city. The tool response came back with three hotels similar to the previous Paris example. Now I need to present this information in a user-friendly way.\\n\\nFirst, I should list each hotel with their details. Use emojis for stars and amenities to make it visually appealing. Maybe bullet points for clarity. I should mention the price, star rating, and key amenities. Also, offer further assistance in case they need more details or booking help. Keep the tone friendly and helpful. Let me check if all the information from the tool response is included. Yep, all three hotels with their respective details. Alright, format that into a clear response.\\n'}, response_metadata={'token_usage': {'completion_tokens': 293, 'prompt_tokens': 503, 'total_tokens': 796, 'completion_time': 1.249299742, 'completion_tokens_details': {'reasoning_tokens': 149}, 'prompt_time': 0.023692794, 'prompt_tokens_details': None, 'queue_time': 0.048058559, 'total_time': 1.272992536}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd6-fdfa-7e81-b0c9-10dfbfdbf0d9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 503, 'output_tokens': 293, 'total_tokens': 796, 'output_token_details': {'reasoning': 149}}), HumanMessage(content='Find hotels in Tokyo', additional_kwargs={}, response_metadata={}, id='2ee264c0-7671-4bcf-b345-9aaddb83a7bf'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user asked for hotels in Tokyo. Let me check the tools available. There\\'s a function called search_hotels that takes a city parameter. I need to call that with \"Tokyo\" as the city. I\\'ll make sure to format the tool call correctly within the XML tags. Let me verify the parameters again to ensure they\\'re correct. Once that\\'s done, the response should list the hotels in Tokyo similar to the previous example. I\\'ll proceed to make the tool call now.\\n', 'tool_calls': [{'id': '4sqw7rssr', 'function': {'arguments': '{\"city\":\"Tokyo\"}', 'name': 'search_hotels'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 127, 'prompt_tokens': 503, 'total_tokens': 630, 'completion_time': 0.204546373, 'completion_tokens_details': {'reasoning_tokens': 101}, 'prompt_time': 0.022790592, 'prompt_tokens_details': None, 'queue_time': 0.04640888, 'total_time': 0.227336965}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-0387-7ae2-a6ff-c1fbca2f4839-0', tool_calls=[{'name': 'search_hotels', 'args': {'city': 'Tokyo'}, 'id': '4sqw7rssr', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 503, 'output_tokens': 127, 'total_tokens': 630, 'output_token_details': {'reasoning': 101}}), ToolMessage(content='Hotels in Tokyo:\\n    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\\n    2. City Inn - 4 star, $180/night, business center\\n    3. Budget Stay - 3 star, $75/night, free wifi', name='search_hotels', id='8b458bd2-b90c-4538-b748-8b2f592a01e4', tool_call_id='4sqw7rssr'), AIMessage(content='Here are some hotels in Tokyo with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Amenities: Spa, pool, gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üõÅ Amenities: Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üõÅ Amenities: Free WiFi  \\n\\nWould you like assistance with booking or more details about any of these? üòä', additional_kwargs={'reasoning_content': \"Okay, the user asked for hotels in Tokyo, and I called the search_hotels function with Tokyo as the city. The tool response came back with three hotels similar to the previous example with London. Now I need to present this information in a user-friendly way. Let me check the previous response to maintain consistency.\\n\\nIn the previous interaction, the assistant listed each hotel with bullet points, including stars, price, and amenities. I should follow the same structure here. Let me make sure to mention each hotel's name, star rating, price per night, and list the amenities. Also, end with a friendly offer to assist further. \\n\\nWait, the user might be looking for more details or have specific needs, so offering further assistance is good. I should also check if there are any errors in the tool response. The hotels listed here are the same as before, which is a bit repetitive, but maybe the tool's example data is fixed. Regardless, present it clearly. Alright, format each hotel with emojis as before and keep the tone helpful.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 355, 'prompt_tokens': 504, 'total_tokens': 859, 'completion_time': 0.684203301, 'completion_tokens_details': {'reasoning_tokens': 211}, 'prompt_time': 0.021501146, 'prompt_tokens_details': None, 'queue_time': 0.047129796, 'total_time': 0.705704447}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-04f8-7d00-8dfa-a8e4e36c837c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 504, 'output_tokens': 355, 'total_tokens': 859, 'output_token_details': {'reasoning': 211}})]\n",
      "New York: ~239 tokens, 5 messages\n",
      "[HumanMessage(content=\"Here is a summary of the conversation to date:\\n\\nError generating summary: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\", additional_kwargs={'lc_source': 'summarization'}, response_metadata={}, id='87181c5e-76ab-4e5a-905f-682203707f11'), HumanMessage(content='Find hotels in New York', additional_kwargs={}, response_metadata={}, id='c40aaef9-8896-4d3b-bf26-08af616ebf52'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user asked for hotels in New York. Let me check the available functions. The only function provided is search_hotels, which requires a city parameter. Since the user specified New York, I need to call that function with \"New York\" as the city. I should make sure the arguments are correctly formatted in JSON. Let me structure the tool_call accordingly.\\n', 'tool_calls': [{'id': '7va83cf7a', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'search_hotels'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 505, 'total_tokens': 606, 'completion_time': 0.387211114, 'completion_tokens_details': {'reasoning_tokens': 75}, 'prompt_time': 0.026624382, 'prompt_tokens_details': None, 'queue_time': 0.159921768, 'total_time': 0.413835496}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-08c2-7231-bb70-015904a81508-0', tool_calls=[{'name': 'search_hotels', 'args': {'city': 'New York'}, 'id': '7va83cf7a', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 505, 'output_tokens': 101, 'total_tokens': 606, 'output_token_details': {'reasoning': 75}}), ToolMessage(content='Hotels in New York:\\n    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\\n    2. City Inn - 4 star, $180/night, business center\\n    3. Budget Stay - 3 star, $75/night, free wifi', name='search_hotels', id='0ea92477-3e00-40f7-9f59-6ac11759c8d5', tool_call_id='7va83cf7a'), AIMessage(content='Here are some hotels in New York with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Spa, üèä\\u200d‚ôÇÔ∏è Pool, üèãÔ∏è Gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üíº Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üåê Free WiFi  \\n\\nWould you like more details about any of these options or assistance with booking?', additional_kwargs={'reasoning_content': \"Okay, the user asked for hotels in New York, and I used the search_hotels function to get the results. The tool response listed three hotels with their details. Now I need to present this information clearly. Let me start by summarizing each hotel's key features. The Grand Hotel is a 5-star option which is more expensive, suitable for those looking for luxury. City Inn is a 4-star with a business center, maybe good for business travelers. Budget Stay is a 3-star with affordable prices and free wifi, which might appeal to budget-conscious guests. I should mention the price ranges and highlight the amenities. Also, I should ask if they need more details or help booking. Make sure the response is friendly and helpful.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 295, 'prompt_tokens': 367, 'total_tokens': 662, 'completion_time': 0.512130176, 'completion_tokens_details': {'reasoning_tokens': 150}, 'prompt_time': 0.018693102, 'prompt_tokens_details': None, 'queue_time': 0.047894108, 'total_time': 0.530823278}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-0b65-7d40-b0d7-c6b90fcb49c8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 367, 'output_tokens': 295, 'total_tokens': 662, 'output_token_details': {'reasoning': 150}})]\n",
      "Dubai: ~333 tokens, 6 messages\n",
      "[HumanMessage(content=\"Here is a summary of the conversation to date:\\n\\nError generating summary: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\", additional_kwargs={'lc_source': 'summarization'}, response_metadata={}, id='fecfc05b-c9ea-4c9d-8a42-16ecf69839e8'), AIMessage(content='Here are some hotels in New York with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Spa, üèä\\u200d‚ôÇÔ∏è Pool, üèãÔ∏è Gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üíº Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üåê Free WiFi  \\n\\nWould you like more details about any of these options or assistance with booking?', additional_kwargs={'reasoning_content': \"Okay, the user asked for hotels in New York, and I used the search_hotels function to get the results. The tool response listed three hotels with their details. Now I need to present this information clearly. Let me start by summarizing each hotel's key features. The Grand Hotel is a 5-star option which is more expensive, suitable for those looking for luxury. City Inn is a 4-star with a business center, maybe good for business travelers. Budget Stay is a 3-star with affordable prices and free wifi, which might appeal to budget-conscious guests. I should mention the price ranges and highlight the amenities. Also, I should ask if they need more details or help booking. Make sure the response is friendly and helpful.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 295, 'prompt_tokens': 367, 'total_tokens': 662, 'completion_time': 0.512130176, 'completion_tokens_details': {'reasoning_tokens': 150}, 'prompt_time': 0.018693102, 'prompt_tokens_details': None, 'queue_time': 0.047894108, 'total_time': 0.530823278}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-0b65-7d40-b0d7-c6b90fcb49c8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 367, 'output_tokens': 295, 'total_tokens': 662, 'output_token_details': {'reasoning': 150}}), HumanMessage(content='Find hotels in Dubai', additional_kwargs={}, response_metadata={}, id='b0a117f0-7cb0-4732-8188-ac094089eb98'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user previously asked for hotels in New York, and I provided a list using the search_hotels function. Now they\\'re asking for hotels in Dubai. I need to call the same function again but with the city parameter set to Dubai. Let me check the tools available. The function search_hotels requires the city as a parameter. There\\'s no other function, so I should generate another tool call for Dubai. Make sure the JSON is correctly formatted with \"Dubai\" as the city value. No other parameters are needed. I\\'ll wrap it in the tool_call tags as specified.\\n', 'tool_calls': [{'id': 'gp9h1d6td', 'function': {'arguments': '{\"city\":\"Dubai\"}', 'name': 'search_hotels'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 506, 'total_tokens': 652, 'completion_time': 0.256740461, 'completion_tokens_details': {'reasoning_tokens': 120}, 'prompt_time': 0.026131936, 'prompt_tokens_details': None, 'queue_time': 0.048013614, 'total_time': 0.282872397}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-0e6c-7161-8eac-65122fc03e84-0', tool_calls=[{'name': 'search_hotels', 'args': {'city': 'Dubai'}, 'id': 'gp9h1d6td', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 506, 'output_tokens': 146, 'total_tokens': 652, 'output_token_details': {'reasoning': 120}}), ToolMessage(content='Hotels in Dubai:\\n    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\\n    2. City Inn - 4 star, $180/night, business center\\n    3. Budget Stay - 3 star, $75/night, free wifi', name='search_hotels', id='c8171e1d-9b99-4728-b888-b7a0c38e5878', tool_call_id='gp9h1d6td'), AIMessage(content='Here are some hotels in Dubai with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Spa, üèä\\u200d‚ôÇÔ∏è Pool, üèãÔ∏è Gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üíº Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üåê Free WiFi  \\n\\nWould you like more details about any of these options or assistance with booking?', additional_kwargs={'reasoning_content': 'Okay, the user asked for hotels in Dubai, and I called the search_hotels function with Dubai as the city. The response from the tool listed three hotels similar to the previous New York example. Now I need to present this information clearly. Let me check the hotels provided:\\n\\n1. Grand Hotel: 5 stars, $350/night, spa, pool, gym.\\n2. City Inn: 4 stars, $180/night, business center.\\n3. Budget Stay: 3 stars, $75/night, free wifi.\\n\\nI should format them in a user-friendly way, maybe using bullet points with emojis for each feature. Also, I should ask if they need more details or booking help. Make sure the response is consistent with the previous one when the user asked about New York. Avoid any markdown and keep it natural.\\n'}, response_metadata={'token_usage': {'completion_tokens': 319, 'prompt_tokens': 505, 'total_tokens': 824, 'completion_time': 0.482630014, 'completion_tokens_details': {'reasoning_tokens': 175}, 'prompt_time': 0.021766467, 'prompt_tokens_details': None, 'queue_time': 0.158664023, 'total_time': 0.504396481}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-104b-71c1-83e7-1ec745825c11-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 505, 'output_tokens': 319, 'total_tokens': 824, 'output_token_details': {'reasoning': 175}})]\n",
      "Singapore: ~336 tokens, 6 messages\n",
      "[HumanMessage(content=\"Here is a summary of the conversation to date:\\n\\nError generating summary: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\", additional_kwargs={'lc_source': 'summarization'}, response_metadata={}, id='574070ae-38e9-4d8d-bb61-feaf9b8ada9b'), AIMessage(content='Here are some hotels in Dubai with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Spa, üèä\\u200d‚ôÇÔ∏è Pool, üèãÔ∏è Gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üíº Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üåê Free WiFi  \\n\\nWould you like more details about any of these options or assistance with booking?', additional_kwargs={'reasoning_content': 'Okay, the user asked for hotels in Dubai, and I called the search_hotels function with Dubai as the city. The response from the tool listed three hotels similar to the previous New York example. Now I need to present this information clearly. Let me check the hotels provided:\\n\\n1. Grand Hotel: 5 stars, $350/night, spa, pool, gym.\\n2. City Inn: 4 stars, $180/night, business center.\\n3. Budget Stay: 3 stars, $75/night, free wifi.\\n\\nI should format them in a user-friendly way, maybe using bullet points with emojis for each feature. Also, I should ask if they need more details or booking help. Make sure the response is consistent with the previous one when the user asked about New York. Avoid any markdown and keep it natural.\\n'}, response_metadata={'token_usage': {'completion_tokens': 319, 'prompt_tokens': 505, 'total_tokens': 824, 'completion_time': 0.482630014, 'completion_tokens_details': {'reasoning_tokens': 175}, 'prompt_time': 0.021766467, 'prompt_tokens_details': None, 'queue_time': 0.158664023, 'total_time': 0.504396481}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-104b-71c1-83e7-1ec745825c11-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 505, 'output_tokens': 319, 'total_tokens': 824, 'output_token_details': {'reasoning': 175}}), HumanMessage(content='Find hotels in Singapore', additional_kwargs={}, response_metadata={}, id='4bc9e1cc-cc6c-44bf-956f-eabb784d1f34'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user previously asked for hotels in Dubai, and I used the search_hotels function to get the results. Now they\\'re asking for hotels in Singapore. I need to follow the same process.\\n\\nFirst, check if there\\'s a function available for searching hotels. The tools provided include the search_hotels function which requires a city parameter. Since the user mentioned Singapore, I should call that function with \"Singapore\" as the city argument.\\n\\nI should make sure there are no errors this time. The previous response had a model decommissioned error, but the function call itself was correct. The user\\'s current request is straightforward, so I just need to trigger the function again with the new city name.\\n\\nNo other functions are available, so the only option is to use search_hotels. I\\'ll format the tool_call properly within the XML tags as specified. Double-check the JSON structure to avoid any syntax issues. Once the function is called, the system should return the hotel data for Singapore, which I can then present to the user in a friendly format.\\n', 'tool_calls': [{'id': 'wpzzwhgvd', 'function': {'arguments': '{\"city\":\"Singapore\"}', 'name': 'search_hotels'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 238, 'prompt_tokens': 504, 'total_tokens': 742, 'completion_time': 0.454046207, 'completion_tokens_details': {'reasoning_tokens': 213}, 'prompt_time': 0.023168995, 'prompt_tokens_details': None, 'queue_time': 0.047231375, 'total_time': 0.477215202}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-133d-7f91-8b94-c5ab9a48ebf5-0', tool_calls=[{'name': 'search_hotels', 'args': {'city': 'Singapore'}, 'id': 'wpzzwhgvd', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 504, 'output_tokens': 238, 'total_tokens': 742, 'output_token_details': {'reasoning': 213}}), ToolMessage(content='Hotels in Singapore:\\n    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\\n    2. City Inn - 4 star, $180/night, business center\\n    3. Budget Stay - 3 star, $75/night, free wifi', name='search_hotels', id='2d3b45df-7573-4251-a2e8-2406783780f0', tool_call_id='wpzzwhgvd'), AIMessage(content='Here are some hotels in Singapore with their details:\\n\\n1. **Grand Hotel**  \\n   - ‚≠ê 5 stars  \\n   - üí∞ $350/night  \\n   - üõÅ Spa, üèä\\u200d‚ôÇÔ∏è Pool, üèãÔ∏è Gym  \\n\\n2. **City Inn**  \\n   - ‚≠ê 4 stars  \\n   - üí∞ $180/night  \\n   - üíº Business center  \\n\\n3. **Budget Stay**  \\n   - ‚≠ê 3 stars  \\n   - üí∞ $75/night  \\n   - üåê Free WiFi  \\n\\nWould you like more details about any of these options or assistance with booking?', additional_kwargs={'reasoning_content': 'Okay, the user asked for hotels in Singapore. I need to check if the tool response has the data. The tool response lists three hotels similar to the previous Dubai example. Let me present them clearly. I should format each hotel with stars, price, and amenities. Also, offer further assistance in case they need more details or booking help. Make sure the response is friendly and concise.\\n'}, response_metadata={'token_usage': {'completion_tokens': 222, 'prompt_tokens': 503, 'total_tokens': 725, 'completion_time': 1.154071946, 'completion_tokens_details': {'reasoning_tokens': 78}, 'prompt_time': 0.022063274, 'prompt_tokens_details': None, 'queue_time': 0.424014546, 'total_time': 1.17613522}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_d58dbe76cd', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3dd7-218c-7af2-89c2-5e5fc3b1888a-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 503, 'output_tokens': 222, 'total_tokens': 725, 'output_token_details': {'reasoning': 78}})]\n"
     ]
    }
   ],
   "source": [
    "# Run test\n",
    "cities = [\"Paris\", \"London\", \"Tokyo\", \"New York\", \"Dubai\", \"Singapore\"]\n",
    "\n",
    "for city in cities:\n",
    "    response = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=f\"Find hotels in {city}\")]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    tokens = count_tokens(response[\"messages\"])\n",
    "    print(f\"{city}: ~{tokens} tokens, {len(response['messages'])} messages\")\n",
    "    print(f\"{(response['messages'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44135d63",
   "metadata": {},
   "source": [
    "### Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2689f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"groq:llama-3.1-8b-instant\",\n",
    "    tools=[search_hotels],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    system_prompt=(\n",
    "        \"You MUST use only the provided tools. \"\n",
    "        \"Do NOT use web search, browser, or brave_search. \"\n",
    "        \"If hotel data is needed, call search_hotels.\"\n",
    "    ),\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"groq:llama-3.1-8b-instant\",\n",
    "            trigger=(\"fraction\", 0.005),\n",
    "            keep=(\"fraction\", 0.002),\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82ba5a",
   "metadata": {},
   "source": [
    "### Human In the Loop MiddleWare\n",
    "Pause agent execution for human approval, editing, or rejection of tool calls before they execute. Human-in-the-loop is useful for the following:\n",
    "\n",
    "High-stakes operations requiring human approval (e.g. database writes, financial transactions).\n",
    "Compliance workflows where human oversight is mandatory.\n",
    "Long-running conversations where human feedback guides the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d4ae4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "def read_email_tool(email_id: str) -> str:\n",
    "    \"\"\"Mock function to read an email by its ID.\"\"\"\n",
    "    return f\"Email content for ID: {email_id}\"\n",
    "\n",
    "def send_email_tool(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Mock function to send an email.\"\"\"\n",
    "    return f\"Email sent to {recipient} with subject '{subject}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7bf4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=create_agent(\n",
    "    model=\"groq:llama-3.1-8b-instant\",\n",
    "    tools=[read_email_tool,send_email_tool],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_email_tool\":{\n",
    "                    \"allowed_decisions\":[\"approve\",\"edit\",\"reject\"]\n",
    "                },\n",
    "                \"read_email_tool\":False,\n",
    "\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4256631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"test-approve\"}}\n",
    "# Step 1: Request\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Send email to john@test.com with subject 'Hello' and body 'How are you?'\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17033796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∏Ô∏è Paused! Approving...\n",
      "‚úÖ Result: Email sent to john@test.com with subject 'Hello'.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "# Step 2: Approve\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"‚è∏Ô∏è Paused! Approving...\")\n",
    "    \n",
    "    result = agent.invoke(\n",
    "        Command(\n",
    "            resume={\n",
    "                \"decisions\": [\n",
    "                    {\"type\": \"approve\"}\n",
    "                ]\n",
    "            }\n",
    "        ),\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Result: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "234b4247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Send email to john@test.com with subject 'Hello' and body 'How are you?'\", additional_kwargs={}, response_metadata={}, id='e092973d-669c-45a6-b910-9afe20538a22'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '534qhey07', 'function': {'arguments': '{\"body\":\"How are you?\",\"recipient\":\"john@test.com\",\"subject\":\"Hello\"}', 'name': 'send_email_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 309, 'total_tokens': 341, 'completion_time': 0.028588123, 'completion_tokens_details': None, 'prompt_time': 0.021160885, 'prompt_tokens_details': None, 'queue_time': 0.045323396, 'total_time': 0.049749008}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3df5-7ff3-7e61-9993-997f952558d0-0', tool_calls=[{'name': 'send_email_tool', 'args': {'body': 'How are you?', 'recipient': 'john@test.com', 'subject': 'Hello'}, 'id': '534qhey07', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 309, 'output_tokens': 32, 'total_tokens': 341}),\n",
       "  ToolMessage(content=\"Email sent to john@test.com with subject 'Hello'\", name='send_email_tool', id='a0edd2b3-eda5-4201-96fe-5d34fb4948e8', tool_call_id='534qhey07'),\n",
       "  AIMessage(content=\"Email sent to john@test.com with subject 'Hello'.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 362, 'total_tokens': 374, 'completion_time': 0.01868228, 'completion_tokens_details': None, 'prompt_time': 0.024530534, 'prompt_tokens_details': None, 'queue_time': 0.046030786, 'total_time': 0.043212814}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3df5-b057-7681-b610-abf72f9e5ae5-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 362, 'output_tokens': 12, 'total_tokens': 374})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503e596",
   "metadata": {},
   "source": [
    "###  Reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f63a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "def read_email_tool(email_id: str) -> str:\n",
    "    \"\"\"Mock function to read an email by its ID.\"\"\"\n",
    "    return f\"Email content for ID: {email_id}\"\n",
    "\n",
    "def send_email_tool(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Mock function to send an email.\"\"\"\n",
    "    return f\"Email sent to {recipient} with subject '{subject}'\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[read_email_tool,send_email_tool],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_email_tool\": {\n",
    "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
    "                },\n",
    "                \"read_email_tool\": False,\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65f61028",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtest-reject\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Step 1: Request\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSend email to john@test.com with subject \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHello\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m and body \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHow are you?\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1183\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1170\u001b[39m request = ModelRequest(\n\u001b[32m   1171\u001b[39m     model=model,\n\u001b[32m   1172\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1178\u001b[39m     runtime=runtime,\n\u001b[32m   1179\u001b[39m )\n\u001b[32m   1181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1182\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1185\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1186\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1154\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1152\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1154\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1156\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5691\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5684\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5685\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5686\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5689\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5690\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5691\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5692\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5693\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5694\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5695\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1386\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1385\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1389\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1390\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1391\u001b[39m ):\n\u001b[32m   1392\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1381\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1375\u001b[39m             response,\n\u001b[32m   1376\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1377\u001b[39m             metadata=generation_info,\n\u001b[32m   1378\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1379\u001b[39m         )\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m         response = raw_response.parse()\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1294\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1285\u001b[39m     warnings.warn(\n\u001b[32m   1286\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1288\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1289\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1290\u001b[39m     )\n\u001b[32m   1291\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1292\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1293\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1067\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1064\u001b[39m             err.response.read()\n\u001b[32m   1066\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "During task with name 'model' and id 'a174166f-691f-d599-c77b-27fb7d2386ea'"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"test-reject\"}}\n",
    "# Step 1: Request\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Send email to john@test.com with subject 'Hello' and body 'How are you?'\")]},\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c086a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Reject\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"‚è∏Ô∏è Paused! Approving...\")\n",
    "    \n",
    "    result = agent.invoke(\n",
    "        Command(\n",
    "            resume={\n",
    "                \"decisions\": [\n",
    "                    {\"type\": \"reject\"}\n",
    "                ]\n",
    "            }\n",
    "        ),\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Result: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37e50ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Send email to john@test.com with subject 'Hello' and body 'How are you?'\", additional_kwargs={}, response_metadata={}, id='e092973d-669c-45a6-b910-9afe20538a22'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '534qhey07', 'function': {'arguments': '{\"body\":\"How are you?\",\"recipient\":\"john@test.com\",\"subject\":\"Hello\"}', 'name': 'send_email_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 309, 'total_tokens': 341, 'completion_time': 0.028588123, 'completion_tokens_details': None, 'prompt_time': 0.021160885, 'prompt_tokens_details': None, 'queue_time': 0.045323396, 'total_time': 0.049749008}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3df5-7ff3-7e61-9993-997f952558d0-0', tool_calls=[{'name': 'send_email_tool', 'args': {'body': 'How are you?', 'recipient': 'john@test.com', 'subject': 'Hello'}, 'id': '534qhey07', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 309, 'output_tokens': 32, 'total_tokens': 341}),\n",
       "  ToolMessage(content=\"Email sent to john@test.com with subject 'Hello'\", name='send_email_tool', id='a0edd2b3-eda5-4201-96fe-5d34fb4948e8', tool_call_id='534qhey07'),\n",
       "  AIMessage(content=\"Email sent to john@test.com with subject 'Hello'.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 362, 'total_tokens': 374, 'completion_time': 0.01868228, 'completion_tokens_details': None, 'prompt_time': 0.024530534, 'prompt_tokens_details': None, 'queue_time': 0.046030786, 'total_time': 0.043212814}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3df5-b057-7681-b610-abf72f9e5ae5-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 362, 'output_tokens': 12, 'total_tokens': 374})]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255396d",
   "metadata": {},
   "source": [
    "###  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac08701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "def read_email_tool(email_id: str) -> str:\n",
    "    \"\"\"Mock function to read an email by its ID.\"\"\"\n",
    "    return f\"Email content for ID: {email_id}\"\n",
    "\n",
    "def send_email_tool(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Mock function to send an email.\"\"\"\n",
    "    return f\"Email sent to {recipient} with subject '{subject}'\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[read_email_tool,send_email_tool],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_email_tool\": {\n",
    "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
    "                },\n",
    "                \"read_email_tool\": False,\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2bd139e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtest-edit\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Step 1: Request (with wrong info)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSend email to wrong@email.com with subject \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m and body \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHello\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1183\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1170\u001b[39m request = ModelRequest(\n\u001b[32m   1171\u001b[39m     model=model,\n\u001b[32m   1172\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1178\u001b[39m     runtime=runtime,\n\u001b[32m   1179\u001b[39m )\n\u001b[32m   1181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1182\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1185\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1186\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1154\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1152\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1154\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1156\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5691\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5684\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5685\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5686\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5689\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5690\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5691\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5692\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5693\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5694\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5695\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1386\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1385\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1389\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1390\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1391\u001b[39m ):\n\u001b[32m   1392\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1381\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1375\u001b[39m             response,\n\u001b[32m   1376\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1377\u001b[39m             metadata=generation_info,\n\u001b[32m   1378\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1379\u001b[39m         )\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m         response = raw_response.parse()\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1294\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1285\u001b[39m     warnings.warn(\n\u001b[32m   1286\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1288\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1289\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1290\u001b[39m     )\n\u001b[32m   1291\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1292\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1293\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yewanth Yadav\\OneDrive\\Desktop\\Dev\\langchain-playground\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1067\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1064\u001b[39m             err.response.read()\n\u001b[32m   1066\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "During task with name 'model' and id 'abcfdcec-34af-9d4a-042b-4f465253ef6b'"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"test-edit\"}}\n",
    "\n",
    "# Step 1: Request (with wrong info)\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Send email to wrong@email.com with subject 'Test' and body 'Hello'\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7805a183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Send email to john@test.com with subject 'Hello' and body 'How are you?'\", additional_kwargs={}, response_metadata={}, id='e092973d-669c-45a6-b910-9afe20538a22'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '534qhey07', 'function': {'arguments': '{\"body\":\"How are you?\",\"recipient\":\"john@test.com\",\"subject\":\"Hello\"}', 'name': 'send_email_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 309, 'total_tokens': 341, 'completion_time': 0.028588123, 'completion_tokens_details': None, 'prompt_time': 0.021160885, 'prompt_tokens_details': None, 'queue_time': 0.045323396, 'total_time': 0.049749008}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3df5-7ff3-7e61-9993-997f952558d0-0', tool_calls=[{'name': 'send_email_tool', 'args': {'body': 'How are you?', 'recipient': 'john@test.com', 'subject': 'Hello'}, 'id': '534qhey07', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 309, 'output_tokens': 32, 'total_tokens': 341}),\n",
       "  ToolMessage(content=\"Email sent to john@test.com with subject 'Hello'\", name='send_email_tool', id='a0edd2b3-eda5-4201-96fe-5d34fb4948e8', tool_call_id='534qhey07'),\n",
       "  AIMessage(content=\"Email sent to john@test.com with subject 'Hello'.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 362, 'total_tokens': 374, 'completion_time': 0.01868228, 'completion_tokens_details': None, 'prompt_time': 0.024530534, 'prompt_tokens_details': None, 'queue_time': 0.046030786, 'total_time': 0.043212814}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3df5-b057-7681-b610-abf72f9e5ae5-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 362, 'output_tokens': 12, 'total_tokens': 374})]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41aba5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Edit and approve\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"‚è∏Ô∏è Paused! Editing...\")\n",
    "    \n",
    "    result = agent.invoke(\n",
    "        Command(\n",
    "            resume={\n",
    "                \"decisions\": [\n",
    "                    {\n",
    "                        \"type\": \"edit\",\n",
    "                        \"edited_action\": {\n",
    "                            \"name\": \"send_email_tool\",      # Tool name\n",
    "                            \"args\": {                   # New arguments\n",
    "                                \"recipient\": \"correct@email.com\",\n",
    "                                \"subject\": \"Corrected Subject\",\n",
    "                                \"body\": \"This was edited by human before sending\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ),\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úèÔ∏è Result: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b5acbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Send email to john@test.com with subject 'Hello' and body 'How are you?'\", additional_kwargs={}, response_metadata={}, id='e092973d-669c-45a6-b910-9afe20538a22'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '534qhey07', 'function': {'arguments': '{\"body\":\"How are you?\",\"recipient\":\"john@test.com\",\"subject\":\"Hello\"}', 'name': 'send_email_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 309, 'total_tokens': 341, 'completion_time': 0.028588123, 'completion_tokens_details': None, 'prompt_time': 0.021160885, 'prompt_tokens_details': None, 'queue_time': 0.045323396, 'total_time': 0.049749008}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3df5-7ff3-7e61-9993-997f952558d0-0', tool_calls=[{'name': 'send_email_tool', 'args': {'body': 'How are you?', 'recipient': 'john@test.com', 'subject': 'Hello'}, 'id': '534qhey07', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 309, 'output_tokens': 32, 'total_tokens': 341}),\n",
       "  ToolMessage(content=\"Email sent to john@test.com with subject 'Hello'\", name='send_email_tool', id='a0edd2b3-eda5-4201-96fe-5d34fb4948e8', tool_call_id='534qhey07'),\n",
       "  AIMessage(content=\"Email sent to john@test.com with subject 'Hello'.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 362, 'total_tokens': 374, 'completion_time': 0.01868228, 'completion_tokens_details': None, 'prompt_time': 0.024530534, 'prompt_tokens_details': None, 'queue_time': 0.046030786, 'total_time': 0.043212814}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3df5-b057-7681-b610-abf72f9e5ae5-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 362, 'output_tokens': 12, 'total_tokens': 374})]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
